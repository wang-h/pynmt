[DEFAULT]
system = Transformer
src_lang = en
trg_lang = tr
id = 03
user = XXXX
category = baseline
prefix = /home/%(User)s/workspace/result/%(src_lang)s-%(trg_lang)s
workspace = %(prefix)s/%(system)s.%(src_lang)s-%(trg_lang)s.%(Category)s.%(id)s
output = %(workspace)s/output
save_log = %(workspace)s/log
save_model = %(workspace)s/model
save_vocab = %(workspace)s/vocab

[Data]
max_seq_len = 100
data_path = /home/%(User)s/workspace/WMT2018/en-tr/SPM
train_dataset = train
valid_dataset = test2017
test_dataset = test2017

[GPU]
use_gpu = [0]

[Embedding]
min_freq = 1
src_embed_dim = 512
trg_embed_dim = 512
#share_vocab = True
#share_embedding = True

[Train]
epochs = 10
train_batch_size = 64
valid_batch_size = 16
max_decrease_steps = 30
report_every = 50

[Optimizer]
lr = 2
optim = Adam
max_grad_norm = 0
decay_method = noam
warmup_steps = 8000
adam_beta2 = 0.98
grad_accum_count = 4

[Network]
dropout = 0.1
enc_num_layers = 4
dec_num_layers = 4
hidden_size = 512
attn_type = general

[Transformer]
num_heads = 8
inner_hidden_size = 2048

[Translate]
test_batch_size = 32
use_beam_search = False
k_best = 1
beam_size = 5
replace_unk = True

